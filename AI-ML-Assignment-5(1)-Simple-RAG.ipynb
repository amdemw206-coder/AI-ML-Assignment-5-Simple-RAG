{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a238ca7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\awubs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\awubs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506219b8",
   "metadata": {},
   "source": [
    "### 1. Knowledge Base (KB) Creation and Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf11c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    }
   ],
   "source": [
    "# Chunk text file, then calculate embeddings by calling model.encode()\n",
    "knowledgeBase = \"C:/Users/awubs/OneDrive/Desktop/AD331/KB.txt\"\n",
    "\n",
    "with open(knowledgeBase, 'r', encoding='utf-8') as f:\n",
    "    # Use .strip() to remove leading/trailing whitespace, and filter out empty lines\n",
    "    knowledge_chunks = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "chunk_embeddings = model.encode(knowledge_chunks)\n",
    "print(chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a06267",
   "metadata": {},
   "source": [
    "### 2. Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b206f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created successfully!\n",
      "Vector dimension: 384\n",
      "Total vectors indexed: 3\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "embedding_dimension = chunk_embeddings.shape[1]  # For 'all-MiniLM-L6-v2'\n",
    "number_of_vectors = len(knowledge_chunks)\n",
    "\n",
    "faiss_data = chunk_embeddings.astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dimension)\n",
    "index.add(faiss_data)\n",
    "\n",
    "print(f\"Index created successfully!\")\n",
    "print(f\"Vector dimension: {index.d}\")\n",
    "print(f\"Total vectors indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa3b17",
   "metadata": {},
   "source": [
    "### 3. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4768d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query: str, k: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Takes a user query, embeds it, and searches the FAISS index to retrieve the\n",
    "    top 'k' most relevant chunks from the knowledge base.\n",
    "    \"\"\"\n",
    "    # 1. Generate Query Embedding\n",
    "    # The [None] is used to add a batch dimension (shape: [1, dimension])\n",
    "    query_embedding = model.encode(query)\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    # 2. Perform Similarity Search (Nearest Neighbor)\n",
    "    # D: Distances (or scores)\n",
    "    # I: Indices of the nearest neighbors in the FAISS index\n",
    "    # FAISS uses L2 distance for IndexFlatL2, where smaller distance = higher similarity\n",
    "    Distances, Indices = index.search(query_vector, k)\n",
    "\n",
    "    # 3. Retrieve the Chunks\n",
    "    retrieved_chunks = []\n",
    "    for i, index_id in enumerate(Indices[0]):\n",
    "        # index_id corresponds to the index in the original 'chunks' list\n",
    "        # Check if index_id is valid (sometimes -1 is returned if k > n_total)\n",
    "        if index_id >= 0:\n",
    "            retrieved_chunks.append(knowledge_chunks[index_id])\n",
    "\n",
    "    print(f\"--- Retrieved Top {len(retrieved_chunks)} Chunks ---\")\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        print(f\"Chunk {i+1} (FAISS Index {Indices[0][i]} | Distance: {Distances[0][i]:.4f}):\")\n",
    "        print(chunk.strip())\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da77429",
   "metadata": {},
   "source": [
    "### 4. Generation (Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a06cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Construct a final prompt for a pre-trained LLM  that includes the original query AND the retrieved context chunks.\n",
    "\n",
    "def create_augmented_prompt(query: str, context: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the final prompt for the LLM by framing the context\n",
    "    and demanding an answer based *only* on that context.\n",
    "    \"\"\"\n",
    "    context_str = \"\\n\".join([f\"--- Context Chunk ---\\n{c}\" for c in context])\n",
    "\n",
    "    # The augmented prompt template\n",
    "    prompt_template = f\"\"\"\n",
    "    --- USER QUESTION ---\n",
    "    {query}\n",
    "\n",
    "    Answer From LLM:\n",
    "    \"\"\"\n",
    "    return prompt_template.strip()\n",
    "\n",
    "# Generate the final prompt\n",
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217d89e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Top 3 Chunks ---\n",
      "Chunk 1 (FAISS Index 2 | Distance: 1.4335):\n",
      "Indrik: A mythical creature from Slavic folklore, the indrik is a six-legged beast that is said to possess a powerful sense of smell, which it uses to locate water sources.\n",
      "--------------------\n",
      "Chunk 2 (FAISS Index 1 | Distance: 1.6237):\n",
      "Baku: A benevolent dream-eating creature from Japanese mythology, the baku is said to consume nightmares, which helps people sleep better.\n",
      "--------------------\n",
      "Chunk 3 (FAISS Index 0 | Distance: 1.6928):\n",
      "Bake-kujira: A ghostly whale from Japanese folklore that appears on the sea, often followed by a host of ghostly fish.\n",
      "--------------------\n",
      "--- USER QUESTION ---\n",
      "    Slavic\n",
      "\n",
      "    Answer From LLM: On the basis of this, you would not be able to answer that question. In particular it is impossible for anyone who cannot understand what they are saying if one does so with a very limited understanding and experience as well (e.-g., in school\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Enter a query:\")\n",
    "retrieved_context = retrieve_chunks(user_question, k=3)\n",
    "final_prompt = create_augmented_prompt(user_question, retrieved_context)\n",
    "final_answer = qa_pipeline(final_prompt, max_new_tokens=50, temperature=0.8, do_sample=True, top_p=0.9, repetition_penalty=1.5)[0]['generated_text']\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514b72c",
   "metadata": {},
   "source": [
    "### 5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e45626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Top 1 Chunks ---\n",
      "Chunk 1 (FAISS Index 1 | Distance: 0.9490):\n",
      "Baku: A benevolent dream-eating creature from Japanese mythology, the baku is said to consume nightmares, which helps people sleep better.\n",
      "--------------------\n",
      "--- USER QUESTION ---\n",
      "    What is the primary function of the Baku?\n",
      "\n",
      "    Answer From LLM: Â  There are a number that must be discussed here. The main one being \"the central role\". I can say with absolute certainty what this position actually stands on, but it's just as much an open question to me why there isn't any official\n"
     ]
    }
   ],
   "source": [
    "test_q1=\"What is the primary function of the Baku?\"\n",
    "retrieved_context = retrieve_chunks(test_q1, k=1)\n",
    "final_prompt = create_augmented_prompt(test_q1, retrieved_context)\n",
    "final_answer = qa_pipeline(final_prompt, max_new_tokens=50, temperature=0.8, do_sample=True, top_p=0.9, repetition_penalty=1.5)[0]['generated_text']\n",
    "\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebbfa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Top 1 Chunks ---\n",
      "Chunk 1 (FAISS Index 2 | Distance: 1.8035):\n",
      "Indrik: A mythical creature from Slavic folklore, the indrik is a six-legged beast that is said to possess a powerful sense of smell, which it uses to locate water sources.\n",
      "--------------------\n",
      "--- USER QUESTION ---\n",
      "    How are you supposed to cook sausages?\n",
      "\n",
      "    Answer From LLM: I'm usually on a quest for the best recipes and have only just come across an excellent recipe. That's not how it works here, except that this is my first time eating one of these...the other day (my last), as soon after\n"
     ]
    }
   ],
   "source": [
    "test_q2=\"How are you supposed to cook sausages?\"\n",
    "retrieved_context = retrieve_chunks(test_q2, k=1)\n",
    "final_prompt = create_augmented_prompt(test_q2, retrieved_context)\n",
    "final_answer = qa_pipeline(final_prompt, max_new_tokens=50, temperature=0.8, do_sample=True, top_p=0.9, repetition_penalty=1.5)[0]['generated_text']\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2586978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Top 2 Chunks ---\n",
      "Chunk 1 (FAISS Index 1 | Distance: 0.9379):\n",
      "Baku: A benevolent dream-eating creature from Japanese mythology, the baku is said to consume nightmares, which helps people sleep better.\n",
      "--------------------\n",
      "Chunk 2 (FAISS Index 2 | Distance: 1.1586):\n",
      "Indrik: A mythical creature from Slavic folklore, the indrik is a six-legged beast that is said to possess a powerful sense of smell, which it uses to locate water sources.\n",
      "--------------------\n",
      "--- USER QUESTION ---\n",
      "    Which creature has a powerful sense of smell, and which creature is known for consuming dreams?\n",
      "\n",
      "    Answer From LLM: The following are the answers to questions 1. Will it be possible (or advisable) if you eat them before waking up in order that they will not wake people when seen by their senses during sleep or from outside view? 2. Can I make an\n"
     ]
    }
   ],
   "source": [
    "test_q3=\"Which creature has a powerful sense of smell, and which creature is known for consuming dreams?\"\n",
    "retrieved_context = retrieve_chunks(test_q3, k=2)\n",
    "final_prompt = create_augmented_prompt(test_q3, retrieved_context)\n",
    "final_answer = qa_pipeline(final_prompt, max_new_tokens=50, temperature=0.8, do_sample=True, top_p=0.9, repetition_penalty=1.5)[0]['generated_text']\n",
    "\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
